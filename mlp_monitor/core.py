# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_core.ipynb.

# %% auto 0
__all__ = ['reg_monitor', 'rec_monitor', 'loss_rec', 'LossMonitor', 'loss_l1', 'MLP']

# %% ../nbs/01_core.ipynb 3
import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from catalyst import utils as ctutils
import seaborn as sns
from .utils import lookahead
import numpy as np

# %% ../nbs/01_core.ipynb 7
import pandas as pd
from functools import wraps

class LossMonitor:
    """Parametrized decorator, name with be used to store loss function."""
    def __init__(self, name):
        self.name = name
        self._losses = []

    @property
    def losses(self):
        return pd.DataFrame.from_dict(self._losses)

    def __call__(self, method):
        """Decorator to log loss functions"""

        @wraps(method)
        def _loss(*method_args, **method_kwargs):
            l = method(*method_args, **method_kwargs)
            self._losses.append({self.name: l.detach().cpu().numpy()})
            return l

        return _loss
    
    def plot(self, ax = None):
        if ax is None:
            ax = plt.gca()
        return ax.plot(range(len(self.losses)), self.losses, label=self.name)



# %% ../nbs/01_core.ipynb 8
reg_monitor = LossMonitor("Reg")
@reg_monitor
def loss_l1(model, factor=0.0005):
    l1_crit = nn.L1Loss(reduction="mean")
    reg_loss = 0
    for param in model.parameters():
        reg_loss += l1_crit(param, torch.zeros(param.size()))

    return factor * reg_loss


rec_monitor = LossMonitor("Rec")
# @rec_monitor
# def loss_rec(x_pred, x_true):
#     return nn.functional.cross_entropy(x_pred, x_true)

loss_rec = rec_monitor(nn.BCEWithLogitsLoss())


# %% ../nbs/01_core.ipynb 10
class MLP(nn.Module):
    def __init__(self, input_dim: int, hidden_dim = 30) -> None:
        super().__init__()
        self.input_dim = input_dim
        seq = [
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        ]
        self.seq = nn.Sequential(*seq)

    def forward(self, x):
        return self.seq(x)


# %% ../nbs/01_core.ipynb 15
from matplotlib import pyplot as plt
