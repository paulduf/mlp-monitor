# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_core.ipynb.

# %% auto 0
__all__ = ['reg_monitor', 'rec_monitor', 'loss_rec', 'LossMonitor', 'loss_l1', 'MLP', 'AdditiveLoss']

# %% ../nbs/01_core.ipynb 3
import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from catalyst import utils as ctutils
import seaborn as sns
from .utils import lookahead
import numpy as np
import matplotlib.pyplot as plt

# %% ../nbs/01_core.ipynb 7
import pandas as pd
from functools import wraps


class LossMonitor:
    """Parametrized decorator, name with be used to store loss function."""

    def __init__(self, name):
        self.name = name
        self._losses = []

    @property
    def losses(self):
        return pd.DataFrame.from_dict(self._losses)

    def __call__(self, method):
        """Decorator to log loss functions"""

        @wraps(method)
        def _loss(*method_args, batch=None, epoch=None, **method_kwargs):
            l = method(*method_args, **method_kwargs)
            self._losses.append(
                {self.name: l.detach().cpu().numpy()} | {"Epoch": epoch, "Batch": batch}
            )
            return l

        _loss.__name__ = self.name
        return _loss

    def plot(self, ax=None):
        if ax is None:
            ax = plt.gca()
        return ax.plot(range(len(self.losses)), self.losses, label=self.name)

    def reset(self):
        self._losses = []

# %% ../nbs/01_core.ipynb 8
reg_monitor = LossMonitor("Reg")
@reg_monitor
def loss_l1(model, factor=0.0005):
    l1_crit = nn.L1Loss(reduction="mean")
    reg_loss = 0
    for param in model.parameters():
        reg_loss += l1_crit(param, torch.zeros(param.size()))

    return factor * reg_loss


rec_monitor = LossMonitor("Rec")
# @rec_monitor
# def loss_rec(x_pred, x_true):
#     return nn.functional.cross_entropy(x_pred, x_true)

loss_rec = rec_monitor(nn.BCEWithLogitsLoss())
loss_rec.__name__

# %% ../nbs/01_core.ipynb 10
class MLP(nn.Module):
    def __init__(self, input_dim: int, hidden_dim = 30) -> None:
        super().__init__()
        self.input_dim = input_dim
        seq = [
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        ]
        self.seq = nn.Sequential(*seq)

    def forward(self, x):
        return self.seq(x)


# %% ../nbs/01_core.ipynb 15
from matplotlib import pyplot as plt

# %% ../nbs/01_core.ipynb 20
class AdditiveLoss:
    def __init__(self, loss_fcts, model):
        self.loss_fcts = loss_fcts
        self.model = model
        self.grads = []

    def __call__(self, loss_args, batch, epoch):
        grads = {}
        losses = []
        for (loss, args),  has_more in lookahead(zip(self.loss_fcts, loss_args)):
            _loss = loss(*args)
            _loss.backward(retain_graph=has_more)
            losses.append(_loss)
            grads[loss.__name__] = get_grad(self.model)
            reset_grad(self.model)
        assign_grad(self.model, [sum(g) for g in zip(*grads.values())])
        self.grads.append({k:norm_grad(g) for k, g in grads.items()} | {"Epoch": epoch, "Batch": batch})
        return sum(losses)
