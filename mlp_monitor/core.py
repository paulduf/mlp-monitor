# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_core.ipynb.

# %% auto 0
__all__ = ['reg_monitor', 'rec_monitor', 'loss_rec', 'LossMonitor', 'loss_l1', 'MLP']

# %% ../nbs/01_core.ipynb 3
import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from catalyst import utils as ctutils
import seaborn as sns
from .utils import lookahead
import numpy as np
import matplotlib.pyplot as plt

# %% ../nbs/01_core.ipynb 7
import pandas as pd
from functools import wraps


class LossMonitor:
    """Parametrized decorator, name with be used to store loss function."""

    def __init__(self, name):
        self.name = name
        self._losses = []

    @property
    def losses(self):
        return pd.DataFrame.from_dict(self._losses)

    def __call__(self, method):
        """Decorator to log loss functions"""

        @wraps(method)
        def _loss(*method_args, batch=None, epoch=None, **method_kwargs):
            l = method(*method_args, **method_kwargs)
            self._losses.append(
                {self.name: l.detach().cpu().numpy()} | {"Epoch": epoch, "Batch": batch}
            )
            return l

        _loss.__name__ = self.name
        return _loss

    def plot(self, ax=None):
        if ax is None:
            ax = plt.gca()
        return ax.plot(range(len(self.losses)), self.losses, label=self.name)

    def reset(self):
        self._losses = []

# %% ../nbs/01_core.ipynb 10
reg_monitor = LossMonitor("Reg")
@reg_monitor
def loss_l1(model, factor=0.0005):
    l1_crit = nn.L1Loss(reduction="mean")
    reg_loss = 0
    for param in model.parameters():
        reg_loss += l1_crit(param, torch.zeros(param.size()))

    return factor * reg_loss


rec_monitor = LossMonitor("Rec")
# @rec_monitor
# def loss_rec(x_pred, x_true):
#     return nn.functional.cross_entropy(x_pred, x_true)

loss_rec = rec_monitor(nn.BCEWithLogitsLoss())
loss_rec.__name__

# %% ../nbs/01_core.ipynb 12
class MLP(nn.Module):
    def __init__(self, input_dim: int, hidden_dim = 30) -> None:
        super().__init__()
        self.input_dim = input_dim
        seq = [
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        ]
        self.seq = nn.Sequential(*seq)

    def forward(self, x):
        return self.seq(x)


# %% ../nbs/01_core.ipynb 17
from matplotlib import pyplot as plt
